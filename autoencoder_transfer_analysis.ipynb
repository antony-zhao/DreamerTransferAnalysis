{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3163fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from dreamer_init_weights import *\n",
    "\n",
    "initial_model_path = \"imagenet_pretrained.ckpt\"\n",
    "early_model = \"autoencoder_transfer_early.ckpt\"\n",
    "later_model = \"autoencoder_transfer_late.ckpt\"\n",
    "\n",
    "\n",
    "initial_model_state = torch.load(initial_model_path, map_location=\"cpu\", weights_only=False)\n",
    "early_model_state = torch.load(early_model, map_location=\"cpu\", weights_only=False)\n",
    "late_model_state = torch.load(later_model, map_location=\"cpu\", weights_only=False)\n",
    "world_model_enc_keys = [key for key in initial_model_state[\"world_model\"].keys() if \"encoder\" in key]\n",
    "world_model_dec_keys = [key for key in initial_model_state[\"world_model\"].keys() if \"decoder\" in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6125bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Encoder Difference\")\n",
    "# for i in range(len(world_model_enc_keys)):\n",
    "#     if initial_model_state[\"world_model\"][world_model_enc_keys[i]].dim() == 1:\n",
    "#         print(f'Layernorm {\"weight\" if \"weight\" in world_model_enc_keys[i] else \"bias\"}:', end=' ')\n",
    "#         print(F.mse_loss(initial_model_state[\"world_model\"][world_model_enc_keys[i]], early_model_state[\"world_model\"][world_model_enc_keys[i]]), end= ' ')\n",
    "#         print(F.mse_loss(initial_model_state[\"world_model\"][world_model_enc_keys[i]], late_model_state[\"world_model\"][world_model_enc_keys[i]]), end=' ')\n",
    "#     else:\n",
    "#         print(\"Conv:\", end=' ')\n",
    "#         print(F.cosine_similarity(initial_model_state[\"world_model\"][world_model_enc_keys[i]], early_model_state[\"world_model\"][world_model_enc_keys[i]]).mean(), end= ' ')\n",
    "#         print(F.cosine_similarity(initial_model_state[\"world_model\"][world_model_enc_keys[i]], late_model_state[\"world_model\"][world_model_enc_keys[i]]).mean(), end=' ')\n",
    "#     print()\n",
    "# print(\"Decoder Difference\")\n",
    "# for i in range(len(world_model_dec_keys)):\n",
    "#     if initial_model_state[\"world_model\"][world_model_dec_keys[i]].dim() == 1:\n",
    "#         print(f'Layernorm {\"weight\" if \"weight\" in world_model_dec_keys[i] else \"bias\"}:', end=' ')\n",
    "#         print(F.mse_loss(initial_model_state[\"world_model\"][world_model_dec_keys[i]], early_model_state[\"world_model\"][world_model_dec_keys[i]]), end= ' ')\n",
    "#         print(F.mse_loss(initial_model_state[\"world_model\"][world_model_dec_keys[i]], late_model_state[\"world_model\"][world_model_dec_keys[i]]), end=' ')\n",
    "#     else:\n",
    "#         print(\"Conv:\", end=' ')\n",
    "#         print(F.cosine_similarity(initial_model_state[\"world_model\"][world_model_dec_keys[i]], early_model_state[\"world_model\"][world_model_dec_keys[i]]).mean(), end= ' ')\n",
    "#         print(F.cosine_similarity(initial_model_state[\"world_model\"][world_model_dec_keys[i]], late_model_state[\"world_model\"][world_model_dec_keys[i]]).mean(), end=' ')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34d3dc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Differences\n",
      "------------------------------------------------------------\n",
      "Layer                          Type       Early Diff    Late Diff\n",
      "------------------------------------------------------------\n",
      "weight                         Conv         0.996398     0.862410\n",
      "weight                         LN           0.000015     0.012105\n",
      "bias                           LN           0.000005     0.014555\n",
      "weight                         Conv         0.993883     0.667888\n",
      "weight                         LN           0.000018     0.015164\n",
      "bias                           LN           0.000003     0.011438\n",
      "weight                         Conv         0.988533     0.605790\n",
      "weight                         LN           0.000031     0.013224\n",
      "bias                           LN           0.000004     0.001908\n",
      "weight                         Conv         0.989789     0.646106\n",
      "weight                         LN           0.000036     0.029807\n",
      "bias                           LN           0.000002     0.000063\n",
      "------------------------------------------------------------\n",
      "\n",
      "Decoder Differences\n",
      "------------------------------------------------------------\n",
      "Layer                          Type       Early Diff    Late Diff\n",
      "------------------------------------------------------------\n",
      "weight                         Conv         0.968634     0.210494\n",
      "bias                           LN           0.000003     0.000195\n",
      "weight                         Conv         0.989530     0.571681\n",
      "weight                         LN           0.000038     0.134731\n",
      "bias                           LN           0.000007     0.159180\n",
      "weight                         Conv         0.981464     0.238599\n",
      "weight                         LN           0.000048     0.105575\n",
      "bias                           LN           0.000060     0.217157\n",
      "weight                         Conv         0.981537     0.508613\n",
      "weight                         LN           0.000041     0.003183\n",
      "bias                           LN           0.000003     0.001028\n",
      "weight                         Conv         0.960538     0.635498\n",
      "bias                           LN           0.000000     0.000396\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def print_layer_diffs_table(keys, initial_state, early_state, late_state, name=\"Model\"):\n",
    "    # generated by chatgpt just for formatting purposes\n",
    "    header = [\"Layer\", \"Type\", \"Early Diff\", \"Late Diff\"]\n",
    "    print(f\"\\n{name} Differences\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{header[0]:<30} {header[1]:<8} {header[2]:>12} {header[3]:>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for key in keys:\n",
    "        layer = key.split('.')[-1][:30]  # truncate to max 30 chars\n",
    "        if initial_state[key].dim() == 1:\n",
    "            diff_type = \"LN\"\n",
    "            early_diff = F.mse_loss(initial_state[key], early_state[key]).item()\n",
    "            late_diff = F.mse_loss(initial_state[key], late_state[key]).item()\n",
    "        else:\n",
    "            diff_type = \"Conv\"\n",
    "            early_diff = F.cosine_similarity(initial_state[key], early_state[key]).mean().item()\n",
    "            late_diff = F.cosine_similarity(initial_state[key], late_state[key]).mean().item()\n",
    "        \n",
    "        print(f\"{layer:<30} {diff_type:<8} {early_diff:12.6f} {late_diff:12.6f}\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print_layer_diffs_table(world_model_enc_keys, initial_model_state[\"world_model\"], early_model_state[\"world_model\"], late_model_state[\"world_model\"], name=\"Encoder\")\n",
    "print_layer_diffs_table(world_model_dec_keys, initial_model_state[\"world_model\"], early_model_state[\"world_model\"], late_model_state[\"world_model\"], name=\"Decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a868b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model_path = \"imagenet_enc_only.ckpt\"\n",
    "early_model = \"encoder_transfer_early.ckpt\"\n",
    "later_model = \"encoder_transfer_late.ckpt\"\n",
    "\n",
    "\n",
    "initial_model_state = torch.load(initial_model_path, map_location=\"cpu\", weights_only=False)\n",
    "early_model_state = torch.load(early_model, map_location=\"cpu\", weights_only=False)\n",
    "late_model_state = torch.load(later_model, map_location=\"cpu\", weights_only=False)\n",
    "world_model_enc_keys = [key for key in initial_model_state[\"world_model\"].keys() if \"encoder\" in key]\n",
    "world_model_dec_keys = [key for key in initial_model_state[\"world_model\"].keys() if \"decoder\" in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c63744ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder Differences\n",
      "------------------------------------------------------------\n",
      "Layer                          Type       Early Diff    Late Diff\n",
      "------------------------------------------------------------\n",
      "weight                         Conv         0.997452     0.867836\n",
      "weight                         LN           0.000010     0.010277\n",
      "bias                           LN           0.000009     0.011791\n",
      "weight                         Conv         0.992937     0.658117\n",
      "weight                         LN           0.000013     0.015325\n",
      "bias                           LN           0.000003     0.020321\n",
      "weight                         Conv         0.988854     0.606595\n",
      "weight                         LN           0.000024     0.010294\n",
      "bias                           LN           0.000003     0.002030\n",
      "weight                         Conv         0.987292     0.599645\n",
      "weight                         LN           0.000023     0.017456\n",
      "bias                           LN           0.000011     0.007431\n",
      "------------------------------------------------------------\n",
      "\n",
      "Decoder Differences\n",
      "------------------------------------------------------------\n",
      "Layer                          Type       Early Diff    Late Diff\n",
      "------------------------------------------------------------\n",
      "weight                         Conv         0.953628     0.194026\n",
      "bias                           LN           0.000003     0.000218\n",
      "weight                         Conv         0.978274     0.336222\n",
      "weight                         LN           0.000045     0.220241\n",
      "bias                           LN           0.000010     0.144119\n",
      "weight                         Conv         0.971311     0.153144\n",
      "weight                         LN           0.000012     0.114388\n",
      "bias                           LN           0.000008     0.068522\n",
      "weight                         Conv         0.983479     0.398440\n",
      "weight                         LN           0.000045     0.003275\n",
      "bias                           LN           0.000006     0.000809\n",
      "weight                         Conv         0.995614     0.747582\n",
      "bias                           LN           0.000000     0.000087\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print(\"encoder MSE\")\n",
    "# for i in range(len(world_model_enc_keys)):\n",
    "#     if initial_model_state[\"world_model\"][world_model_enc_keys[i]].dim() == 1:\n",
    "#         continue\n",
    "#     print(F.cosine_similarity(initial_model_state[\"world_model\"][world_model_enc_keys[i]], early_model_state[\"world_model\"][world_model_enc_keys[i]]).mean(), end= ' ')\n",
    "#     print(F.cosine_similarity(initial_model_state[\"world_model\"][world_model_enc_keys[i]], late_model_state[\"world_model\"][world_model_enc_keys[i]]).mean(), end=' ')\n",
    "#     scale1 = torch.mean(torch.abs(initial_model_state[\"world_model\"][world_model_enc_keys[i]]))\n",
    "#     scale2 = torch.mean(torch.abs(early_model_state[\"world_model\"][world_model_enc_keys[i]]))\n",
    "#     scale3 = torch.mean(torch.abs(late_model_state[\"world_model\"][world_model_enc_keys[i]]))\n",
    "#     print(f\"scale: {scale1:.4f}, {scale2:.4f}, {scale3:.4f}\")\n",
    "# print(\"decoder MSE\")\n",
    "# for i in range(len(world_model_dec_keys)):\n",
    "#     if initial_model_state[\"world_model\"][world_model_dec_keys[i]].dim() == 1:\n",
    "#         continue\n",
    "#     print(F.cosine_similarity(initial_model_state[\"world_model\"][world_model_dec_keys[i]], early_model_state[\"world_model\"][world_model_dec_keys[i]]).mean(), end= ' ')\n",
    "#     print(F.cosine_similarity(initial_model_state[\"world_model\"][world_model_dec_keys[i]], late_model_state[\"world_model\"][world_model_dec_keys[i]]).mean(), end=' ')\n",
    "#     scale1 = torch.mean(torch.abs(initial_model_state[\"world_model\"][world_model_dec_keys[i]]))\n",
    "#     scale2 = torch.mean(torch.abs(early_model_state[\"world_model\"][world_model_dec_keys[i]]))\n",
    "#     scale3 = torch.mean(torch.abs(late_model_state[\"world_model\"][world_model_dec_keys[i]]))\n",
    "#     print(f\"scale: {scale1:.4f}, {scale2:.4f}, {scale3:.4f}\")\n",
    "\n",
    "\n",
    "print_layer_diffs_table(world_model_enc_keys, initial_model_state[\"world_model\"], early_model_state[\"world_model\"], late_model_state[\"world_model\"], name=\"Encoder\")\n",
    "print_layer_diffs_table(world_model_dec_keys, initial_model_state[\"world_model\"], early_model_state[\"world_model\"], late_model_state[\"world_model\"], name=\"Decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93943fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
